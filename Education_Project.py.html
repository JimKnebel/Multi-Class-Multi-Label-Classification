#!/usr/bin/env python
# coding: utf-8

# <h1 id="tocheading">Table of Contents</h1>
# <div id="toc"></div>

# # Introduction

# ## Aim of this Notebook
# 
# The current notebook serves as both a showcase of my current ability aswell as a playground for testing out new concepts. It is the first of a series consisting of 2 parts: a Machine Learning project and a Data Visualization project (Pokemon). The markdown sections will include but are not limited to the motivations and thought processes behind the code.  

# ## Box-Plots for Education (DrivenData)
# 
# The data used in this project has been retrieved from DrivenData's competition database after their relaunch of the Box-Plots for Education competition. The aim of the competition is to help schools correctly identify spending labels (previously done manually). As you will see in the next chapters, the project is a supervised multi-label multi-class classification problem where the resulting machine will present the most likely budget line items for any given new instance. The aim is to predict the probability that a certain label is attached to a budget line item.

# # 1. Preparation
# 
# First we will make sure to load all the data, import the most basic packages we will likely use and take a quick look at the data frame we end up with.

# In[1]:


# To be viable for Python 2 & 3
from __future__ import division, print_function, unicode_literals

# Common imports, standard procedure
import numpy as np

# To plot pretty figures in jupyter
get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib as mpl
import matplotlib.pyplot as plt

# To have the random generated output be stable across runs
np.random.seed(42)

# Opening up a path to the datafiles and saving the data to a pandas dataframe
import os
import pandas as pd
notebook_path = os.path.abspath("Education_Python.ipynb")
csv = os.path.join(os.path.dirname(notebook_path), "TrainingData.csv")
with open(csv) as file:
    df = pd.read_csv(file, index_col=0)


# Checking the results
print('---------------------------------------------------------')
print('\t\tTraining Dataframe')
print('---------------------------------------------------------')

print(df.head(5))


# # 2. Taking a peek (EDA)
# 
# Now that we have our data, let's get our hands dirty. First we would like to know what we are working with. Not only looking at the shape of the dataframe but also the datatypes, value counts, and plot a couple of distributions to further explore certain columns.
# 

# In[2]:


# Examine the shape and size
df.info()
df.shape


# The dataset is mostly filled with object datatypes. We will later change most datatypes to categorical for computational efficiency. There are 2 columns with floats in it: FTE and Total. When refering to DrivenData's explanation on their site; FTE is the the percentage of full-time work an employee engages in and Total contains the total costs of the noted expenditure. Lets take a look at the contents of the object columns and then inspect the numeric information FTE and Total provide.

# In[3]:


# Print the summary statistics and valuecounts of the columns to look for any irregularities
print('---------------------------------------------------------')
print('\t\tDescribe')
print('---------------------------------------------------------')

print(df.describe())

print('---------------------------------------------------------')
print('\t\tValue Counts')
print('---------------------------------------------------------')

Columns = df.columns.tolist()
for c in Columns:
    print(df[c].value_counts())


# The object data are mostly strings containing many descriptions. As noted by DrivenData these are the features we will be working with. Next we will explore the FTE and Total to see if there is anything noteworthy.

# In[4]:


# Create the histogram of FTE
plt.hist(df['FTE'].dropna())
plt.title('Histogram showing distribution of percentage of full-time work')
plt.xlabel('% of full-time work')
plt.ylabel('Number of employees')
plt.yscale('log')
plt.show()


# It seems like most employees are working less than 10% of full-time work. Now we don't really know why this is. However my guess is that apart from regular employees the dataset also includes the expenditures on third party hiring: substitute teachers, coaches, electricians, plummers, technicians, doctor's etc.

# In[60]:


print('---------------------------------------------------------')
print('\t\tFTE lower than 10%')
print('---------------------------------------------------------')

FTE_10 = df[df['FTE']<0.10]
print(FTE_10['Position_Type'].value_counts())

print('---------------------------------------------------------')
print('\t\tFTE greater than 10%')
print('---------------------------------------------------------')

FTE_rest = df[df['FTE']>=0.10]
print(FTE_rest['Position_Type'].value_counts())


# As it turns out, the people who work less than 10% of full-time work mostly consist of Substitute teachers. 
# </b> Now lets take a quick glance at Total. It seems to be normally distributed with both negative and positive values refering to incomming and outgoing payments. 

# In[36]:


# create the histogram of Total
plt.hist(df['Total'].dropna())
plt.title('Histogram showing the distribution of Total dollars exchanged')
plt.xlabel('Dollars exchanged')
plt.ylabel('Number of instances')
plt.xscale('linear')
plt.yscale('log')
plt.show()


# Lastly, We will take a look at our labels. As described by DrivenData the Labels consist of 9 columns with a plethora of different values/sublabels. We will change the datatypes to categorical for computational efficiency and get an estimate understanding of the size of different options.

# In[46]:


# Time to split the labels from the features, first we make a list containing all the labels:
LABELS = ['Function',
 'Use',
 'Sharing',
 'Reporting',
 'Student_Type',
 'Position_Type',
 'Object_Type',
 'Pre_K',
 'Operating_Status']

# changing the object datatypes to category
df[LABELS] = df[LABELS].apply(lambda x: x.astype('category'), axis=0)

# Calculate number of unique values for each label: num_unique_labels
unique_labels = df[LABELS].apply(pd.Series.nunique)

# Plot number of unique values for each label
unique_labels.plot(kind='bar')
plt.xlabel('Labels')
plt.ylabel('Number of unique values')
plt.show()

total_labels = 'The sum of different labels: %f)' % (unique_labels.sum())
print(total_labels)


# # Modeling

# Finally we arrive at the task not a single person could ever dislike: looking at Models. As mentioned in the introduction this project is an ongoing process that I plan to update whenever I find exciting new ways to upgrade the current model. Therefore there is code woven in between te usual explanations marked with '#' that serves the purpose of giving a glimpse of possible changes in the near future. For now, might the hurt the eyes too much, feel free to actively ignore them.
# </b>
# The following code will execute the multilabel_train_test_split code which does as the name suggests: a train_test_split function that takes into consideration the less common values. The function is stored in the same directory as the current notebook and belongs to DataCamp's warehouse of custom functions.

# In[6]:


# Importing Multilabel functions
import Multilabel

# Get the dummy encoding of the LABELS
dummy_LABELS = pd.get_dummies(df[LABELS])

# Get the columns that are features in the original df
NON_LABELS = [x for x in df.columns if x not in LABELS]

X_train, X_test, y_train, y_test = Multilabel.multilabel_train_test_split(df[NON_LABELS],
                                                               dummy_LABELS,
                                                               0.2)


# In[7]:


get_ipython().run_cell_magic('time', '', 'from sklearn import model_selection\n\n# Imports pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n\n# Import the different algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n# Import feature engineering tools\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import chi2, SelectKBest\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Import preprocessing tools\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n\n# Creating Function for combining all text strings that aren\'t numeric predictors or labels\nnum_data = df.loc[:, [\'FTE\', \'Total\']].fillna(-1000)\nnum_cols = num_data.columns.to_list()\n\ndef combine_text (data_frame, to_drop=num_cols + LABELS):\n    to_drop = set(to_drop) & set(data_frame.columns.tolist())\n    text_data = data_frame.drop(to_drop, axis=1)\n    text_data.fillna(\'\', inplace=True)\n    return text_data.apply(lambda x: " ".join(x), axis=1)\n\n# Setting up FunctionTransformer for preprocessing the text and numerical data seperately\ntxtdata = FunctionTransformer(combine_text, validate=False)\nnumdata = FunctionTransformer(lambda x: x[num_cols], validate=False)\n\n# \nTOKENS_ALPHANUMERIC = \'[A-Za-z0-9]+(?=\\\\s+)\'\n\n# setting the parameters for the chi squared test\n#chi_k = 300\n\n# defining variables to be able to execute the \nmodels = []\nmodels.append((\'LR\', LogisticRegression()))\n#models.append((\'KNN\', KNeighborsClassifier()))\n#models.append((\'DT\', DecisionTreeClassifier()))\n#models.append((\'SVM\', SVC()))\n\nresults = []\nnames = []\nscoring = \'accuracy\'\n\n# Pipeline: Preprocessing and Model execution\n# evaluate each model in turn\nfor name, model in models:\n    pl = Pipeline([\n    (\'union\', FeatureUnion(\n        transformer_list = [\n            (\'numeric_features\', Pipeline([\n                (\'selector\', numdata),\n                (\'imputer\', SimpleImputer())\n                ])),\n            (\'text_features\', Pipeline([\n                    (\'selector\', txtdata),\n                    (\'vectorizer\', CountVectorizer(token_pattern=TOKENS_ALPHANUMERIC,\n                                                   ngram_range=(1,2))),\n                    #(\'dim_red\', SelectKBest(chi2, chi_k))\n                ]))\n             ]\n        )),\n    #(\'int\', PolynomialFeatures(degree=2)),\n    (\'scale\', MaxAbsScaler()),\n    (\'clf\', OneVsRestClassifier(model))\n    ])\n    kfold = model_selection.KFold(n_splits=10)\n    cv_results = model_selection.cross_val_score(pl, X_train, y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n    \n    \n\n')


# # Results
# 
# At first I was worried running a cross validation test in a multi-class multi-label classification machine might cause some issues. However, I tested the pipeline with and without cross validation and the results are very similar. 

# ## Logistic regression without cross validation:
# Accuracy of LR:  0.9170320404721753
# <br>
# CPU times: user 47min 48s, sys: 45.4 s, total: 48min 34s
# <br>
# Wall time: 12min 21s
# 
# ## Logistic regression with Cross Validation:
# LR: 0.918397 (0.002229)
# <br>
# CPU times: user 6h 41min 45s, sys: 6min 25s, total: 6h 48min 10s
# <br>
# Wall time: 1h 43min 51s

# ## Boxplot of models

# In[49]:


# plotting of future models and comparing the accuracy of all models in a boxplot
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()


# # Discussion

# I am very happy to see a working model with an accuracy of around 92% with a small standard deviation. I did try to include more analyses, however a simple kfold of the Logistic Regression already took 2 hours. However I am pleasantly surprised that a simple model like Logistic Regression works perfectly fine even without additional tools.

# # Future changes

# things I will be looking for to add/change in the near future:
# - trying out more models: Naive Bayes, SVM, KNN, Decision Trees
# - include dimensionality reduction, such as a chisquared test: SelectKbest
# - including polynomial features
# - include Gridsearch to inspect and optimize parameters

# In[1]:


get_ipython().run_cell_magic('javascript', '', "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')")


# In[ ]:




